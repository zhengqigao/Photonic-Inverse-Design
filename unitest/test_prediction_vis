import argparse
import os
from typing import Callable, Dict, Iterable, List, Tuple
import torch.cuda.amp as amp
import mlflow
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from pyutils.config import configs
from pyutils.general import AverageMeter, logger as lg
from pyutils.loss import KLLossMixed
from pyutils.torch_train import (
    BestKModelSaver,
    count_parameters,
    get_learning_rate,
    load_model,
    set_torch_deterministic,
)
from pyutils.typing import Criterion, DataLoader, Optimizer, Scheduler
import torch.fft
from core import builder
from core.datasets.mixup import Mixup, MixupAll
from core.utils import DeterministicCtx, normalize, make_axes_locatable
import matplotlib.pyplot as plt
from torch import Tensor

def plot_compare_local(
    epsilon: Tensor,
    input_fields: Tensor,
    pred_fields: Tensor,
    target_fields: Tensor,
    filepath: str,
    pol: str = "Ez",
    norm: bool = True,
    max_fields=32,
    if_colorbar = False,
    field_range = 0.25,
    error_range = 0.1,
) -> None:

    field_val = pred_fields.data.transpose(-1, -2).cpu().numpy()
    target_field_val = target_fields.data.transpose(-1, -2).cpu().numpy()
    input_field_val = input_fields.data.transpose(-1, -2).cpu().numpy()
    # eps_r = simulation.eps_r
    eps_r = epsilon.data.transpose(-1, -2).cpu().numpy()
    err_field_val = field_val - target_field_val
    # field_val = np.abs(field_val)
    field_val = field_val
    # target_field_val = np.abs(target_field_val)
    target_field_val = target_field_val
    err_field_val = np.abs(err_field_val)
    outline_val = np.abs(eps_r)

    # vmax = field_val.max()
    vmin = 0.0
    b = min(max(field_val.shape[0], input_field_val.shape[0]), max_fields)
    target_start = max(-field_val.shape[0], -max_fields)
    input_start = max(-input_field_val.shape[0], -max_fields)

    field_val = field_val[target_start:]
    target_field_val = target_field_val[target_start:]
    err_field_val = err_field_val[target_start:]
    input_field_val = input_field_val[input_start:]
    fig, axes = plt.subplots(4, b, constrained_layout=True, figsize=(3 * b, 8.1))
    if b == 1:
        axes = axes[..., np.newaxis]
    # cmap = "magma"
    cmap = "RdBu_r"
    # print(field_val.shape, target_field_val.shape, outline_val.shape)
    zeros = torch.zeros_like(target_fields[0]).cpu().numpy()
    for i in range(b):
        vmax = np.max(np.abs(target_field_val[i])) if i < target_field_val.shape[0] else 0.1
        h0 = axes[0, i].imshow(input_field_val[i] if i < input_field_val.shape[0] else zeros, cmap=cmap, vmin=-vmax, vmax=vmax, origin="lower")
        if norm:
            h1 = axes[2, i].imshow(normalize(field_val[i]) if i < field_val.shape[0] else zeros, cmap=cmap, origin="lower")
            h2 = axes[1, i].imshow(normalize(target_field_val[i]) if i < target_field_val.shape[0] else zeros, cmap=cmap, origin="lower")
        else:
            h1 = axes[2, i].imshow(field_val[i] if i < field_val.shape[0] else zeros, cmap=cmap, vmin=-field_range, vmax=field_range, origin="lower")
            h2 = axes[1, i].imshow(target_field_val[i] if i < target_field_val.shape[0] else zeros, cmap=cmap, vmin=-field_range, vmax=field_range, origin="lower")
        h3 = axes[3, i].imshow(err_field_val[i] if i < err_field_val.shape[0] else zeros, cmap=cmap, vmin=-error_range, vmax=error_range, origin="lower")
        if if_colorbar:
            for j in range(4):
                divider = make_axes_locatable(axes[j, i])
                cax = divider.append_axes("right", size="5%", pad=0.05)

                fig.colorbar([h0, h1, h2, h3][j], label=pol, ax=axes[j, i], cax=cax)
 
        # fig.colorbar(h2, label=pol, ax=axes[1,i])
        # fig.colorbar(h3, label=pol, ax=axes[2,i])

    # Do black and white so we can see on both magma and RdBu
    for ax in axes.flatten():
        ax.contour(outline_val[0], levels=2, linewidths=1.0, colors="w")
        ax.contour(outline_val[0], levels=2, linewidths=0.5, colors="k")
        ax.set_xticks([])
        ax.set_yticks([])
    # set_ms()
    plt.savefig(filepath, dpi=150)
    plt.close()

def prediction_visualization(model, cfg, test_loader, device, path = './plot/prediction_visualization'):
    print("plot_prediction_visualization is called")
    model.eval()
    # meta is 208, 66
    idx = 66
    with DeterministicCtx(42):
        for batch_idx, (raw_data, raw_target) in enumerate(test_loader):
            for key, d in raw_data.items():
                raw_data[key] = d.to(device, non_blocking=True)
            for key, t in raw_target.items():
                raw_target[key] = t.to(device, non_blocking=True)
            if batch_idx == idx:
            

                data = torch.cat([raw_data["eps"], raw_data["Ez"], raw_data["source"]], dim=1)
                target = raw_target["Ez"]

                target = target.to(device, non_blocking=True)
                output, normalization_factor = model(data, target, grid_step=raw_data["grid_step"], src_mask=raw_data["src_mask"], padding_mask=raw_data["padding_mask"])
                target = target[:, ::40, ...]
                prediction = output*normalization_factor
                prediction = prediction[:, ::40, ...]
                # padding to square:
                if 'mmi' in cfg.dataset.processed_dir:
                    target_size = 256
                elif 'mrr' in cfg.dataset.processed_dir:
                    target_size = 256
                elif 'meta' in cfg.dataset.processed_dir:
                    target_size = 168
                else:
                    raise ValueError("Unsupported dataset")
                print("this is the target size: ", target_size)
                if 'kno' in cfg.model.name.lower():
                    pass
                elif 'fno' in cfg.model.name.lower():
                    pass
                elif 'neurolight' in cfg.model.name.lower():
                    pass
                elif 'cnn' or 'conv' in cfg.model.name.lower():
                    height_padding_top = (target_size - target.shape[-2]) // 2
                    height_padding_bot = target_size - target.shape[-2] - height_padding_top
                    width_padding_left = (target_size - target.shape[-1]) // 2
                    width_padding_right = target_size - target.shape[-1] - width_padding_left
                    target = torch.nn.functional.pad(target, (width_padding_left, width_padding_right, height_padding_top, height_padding_bot), mode='constant', value=0)
                    prediction = torch.nn.functional.pad(prediction, (width_padding_left, width_padding_right, height_padding_top, height_padding_bot), mode='constant', value=0)
                    data = torch.nn.functional.pad(data, (width_padding_left, width_padding_right, height_padding_top, height_padding_bot), mode='constant', value=0)
                elif 'sine' in cfg.model.name.lower():
                    pass
                else:
                    raise ValueError("Unsupported model")
                print("begin to plot the prediction visualization...")
                plot_compare_local(
                    epsilon=data[0, 0:1],
                    input_fields=data[0, 1 : -target.shape[1]],
                    pred_fields=prediction[0],
                    target_fields=target[0],
                    filepath=f"{path}/{cfg.model.name.lower()}-{cfg.dataset.device_list[0]}-colorb_id-{batch_idx}.png",
                    pol="Ez",
                    norm=False,
                    max_fields=4,
                    if_colorbar=True,
                    error_range=0.02,
                    field_range=0.15,
                )
                plot_compare_local(
                    epsilon=data[0, 0:1],
                    input_fields=data[0, 1 : -target.shape[1]],
                    pred_fields=prediction[0],
                    target_fields=target[0],
                    filepath=f"{path}/{cfg.model.name.lower()}-{cfg.dataset.device_list[0]}_id-{batch_idx}.png",
                    pol="Ez",
                    norm=False,
                    max_fields=4,
                    if_colorbar=False,
                    error_range=0.02,
                    field_range=0.15,
                )
                print("plot_prediction_visualization is done")
                print("file path: ", f"{path}/{cfg.model.name.lower()}-{cfg.dataset.device_list[0]}.png")
                break
            else:
                continue

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("config", metavar="FILE", help="config file")
    # parser.add_argument('--run-dir', metavar='DIR', help='run directory')
    # parser.add_argument('--pdb', action='store_true', help='pdb')
    args, opts = parser.parse_known_args()

    configs.load(args.config, recursive=True)
    configs.update(opts)

    if torch.cuda.is_available() and int(configs.run.use_cuda):
        torch.cuda.set_device(configs.run.gpu_id)
        device = torch.device("cuda:" + str(configs.run.gpu_id))
        torch.backends.cudnn.benchmark = True
    else:
        device = torch.device("cpu")
        torch.backends.cudnn.benchmark = False

    if "backbone_cfg" in configs.model.keys():
        if configs.model.backbone_cfg.conv_cfg.type == "Conv2d" or configs.model.backbone_cfg.conv_cfg.type == "LargeKernelConv2d":
            if "r" in configs.model.backbone_cfg.conv_cfg.keys():
                del configs.model.backbone_cfg.conv_cfg["r"]
            if "is_causal" in configs.model.backbone_cfg.conv_cfg.keys():
                del configs.model.backbone_cfg.conv_cfg["is_causal"]
            if "mask_shape" in configs.model.backbone_cfg.conv_cfg.keys():
                del configs.model.backbone_cfg.conv_cfg["mask_shape"]
            if "enable_padding" in configs.model.backbone_cfg.conv_cfg.keys():
                del configs.model.backbone_cfg.conv_cfg["enable_padding"]

    model = builder.make_model(
        device,
        int(configs.run.random_state) if int(configs.run.deterministic) else None,
    )
    lg.info(model)

    _, _, test_loader = builder.make_dataloader()

    load_model(
        model,
        configs.checkpoint.restore_checkpoint,
        ignore_size_mismatch=int(configs.checkpoint.no_linear),
    )
    print("model loaded successfully!", flush=True)
    prediction_visualization(model, configs, test_loader, device)

if __name__ == "__main__":
    main()